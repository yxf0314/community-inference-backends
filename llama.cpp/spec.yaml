backend_name: llama.cpp
version_configs:
  cuda:
    image_name: ghcr.io/ggml-org/llama.cpp:server-cuda
    custom_framework: cuda
  cpu:
    image_name: ghcr.io/ggml-org/llama.cpp:server
    custom_framework: cpu
  vulkan:
    image_name: ghcr.io/ggml-org/llama.cpp:server-vulkan
    custom_framework: rocm
  musa:
    image_name: ghcr.io/ggml-org/llama.cpp:server-musa
    custom_framework: musa
  rocm:
    image_name: ghcr.io/ggml-org/llama.cpp:server-rocm
    custom_framework: rocm
default_version: cpu
default_run_command: -m {{model_path}} --host 0.0.0.0 --port {{port}} --alias {{model_name}}
is_built_in: false
health_check_path: /v1/models
framework_index_map:
  cuda:
    - cuda
  cpu:
    - cpu
  rocm:
    - rocm
    - vulkan
  musa:
    - musa
